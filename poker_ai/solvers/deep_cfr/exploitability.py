"""
Exploitability calculation for Deep CFR.

This module implements best response calculation and exploitability measurement
to validate the quality of learned strategies.
"""

import numpy as np
import torch
import logging
from typing import Dict, List, Tuple, Optional, Any
from collections import defaultdict
import random

from poker_ai.solvers.deep_cfr.info_set_manager import InfoSetManager


class BestResponseCalculator:
    """
    Calculates best response strategies and exploitability for poker agents.
    
    Exploitability is the amount a strategy can be exploited by an optimal counter-strategy.
    Lower exploitability indicates a stronger, more balanced strategy.
    """
    
    def __init__(self, env, num_samples: int = 1000):
        """
        Initialize best response calculator.
        
        Args:
            env: Poker environment
            num_samples: Number of Monte Carlo samples for best response estimation
        """
        self.env = env
        self.num_samples = num_samples
        self.logger = logging.getLogger(__name__)
        
        # Cache for computed best responses
        self._best_response_cache = {}
        self._exploitability_cache = {}
        
    def calculate_exploitability(self, 
                                target_agent, 
                                info_set_manager: Optional[InfoSetManager] = None,
                                use_external_regrets: bool = True) -> float:
        """
        Calculate exploitability of a strategy using Monte Carlo best response.
        
        Args:
            target_agent: Agent whose strategy we're measuring exploitability for
            info_set_manager: External regret storage (if using external regrets)
            use_external_regrets: Whether to use external regrets or network strategy
            
        Returns:
            Exploitability value (lower is better)
        """
        try:
            total_exploitability = 0.0
            valid_samples = 0
            
            # Calculate exploitability from each player position
            for player_position in range(self.env.num_players):
                player_exploitability = self._calculate_player_exploitability(
                    target_agent, 
                    player_position,
                    info_set_manager,
                    use_external_regrets
                )
                
                if player_exploitability is not None:
                    total_exploitability += player_exploitability
                    valid_samples += 1
            
            if valid_samples == 0:
                self.logger.warning("No valid samples for exploitability calculation")
                return float('inf')
            
            # Average exploitability across all positions
            avg_exploitability = total_exploitability / valid_samples
            
            self.logger.debug(f"Calculated exploitability: {avg_exploitability:.6f}")
            return avg_exploitability
            
        except Exception as e:
            self.logger.error(f"Error calculating exploitability: {e}")
            return float('inf')
    
    def _calculate_player_exploitability(self, 
                                       target_agent,
                                       player_position: int,
                                       info_set_manager: Optional[InfoSetManager],
                                       use_external_regrets: bool) -> Optional[float]:
        """
        Calculate exploitability for a specific player position.
        
        Args:
            target_agent: Target agent
            player_position: Position being exploited (0, 1, 2)
            info_set_manager: External regret storage
            use_external_regrets: Whether to use external regrets
            
        Returns:
            Exploitability for this position
        """
        player_utilities = []
        
        for sample in range(self.num_samples):
            try:
                # Run one game with best response against target strategy
                utility = self._run_best_response_game(
                    target_agent,
                    player_position,
                    info_set_manager,
                    use_external_regrets
                )
                
                if utility is not None:
                    player_utilities.append(utility)
                    
            except Exception as e:
                self.logger.debug(f"Error in game sample {sample}: {e}")
                continue
        
        if not player_utilities:
            return None
        
        # Exploitability = average utility of best response
        avg_utility = np.mean(player_utilities)
        return max(0.0, avg_utility)  # Exploitability should be non-negative
    
    def _run_best_response_game(self,
                               target_agent,
                               exploiting_player: int,
                               info_set_manager: Optional[InfoSetManager],
                               use_external_regrets: bool) -> Optional[float]:
        """
        Run a single game where one player uses best response against target strategy.
        
        Args:
            target_agent: Agent with strategy being exploited
            exploiting_player: Player index using best response (0, 1, 2)
            info_set_manager: External regret storage
            use_external_regrets: Whether to use external regrets
            
        Returns:
            Utility for the exploiting player
        """
        try:
            # Reset environment
            observations = self.env.reset()
            done = False
            game_history = []
            
            while not done:
                current_player = self.env.current_player()
                if current_player is None:
                    break
                
                # Get current player index
                if hasattr(self.env, 'agents') and current_player in self.env.agents:
                    current_player_idx = self.env.agents.index(current_player)
                else:
                    break
                
                # Get legal actions
                legal_actions = self.env.get_legal_actions(current_player)
                if not legal_actions:
                    break
                
                # Choose action based on strategy
                if current_player_idx == exploiting_player:
                    # Use best response strategy (simplified: greedy action selection)
                    action = self._get_best_response_action(
                        observations.get(current_player, {}),
                        legal_actions,
                        target_agent,
                        info_set_manager,
                        use_external_regrets
                    )
                else:
                    # Use target agent's strategy
                    action = self._get_target_strategy_action(
                        target_agent,
                        observations.get(current_player, {}),
                        legal_actions,
                        info_set_manager,
                        use_external_regrets
                    )
                
                # Store game state for analysis
                game_history.append({
                    'player': current_player,
                    'player_idx': current_player_idx,
                    'action': action,
                    'legal_actions': legal_actions.copy(),
                    'observation': observations.get(current_player, {}).copy()
                })
                
                # Execute action
                observations, rewards, done, info = self.env.step(action)
                
                if done:
                    break
            
            # Get final rewards
            if hasattr(self.env, 'get_final_rewards'):
                final_rewards = self.env.get_final_rewards()
            else:
                final_rewards = rewards if rewards else {}
            
            # Return utility for exploiting player
            exploiting_player_name = self.env.agents[exploiting_player] if exploiting_player < len(self.env.agents) else None
            if exploiting_player_name and exploiting_player_name in final_rewards:
                return final_rewards[exploiting_player_name]
            else:
                return 0.0
                
        except Exception as e:
            self.logger.debug(f"Error in best response game: {e}")
            return None
    
    def _get_best_response_action(self,
                                 observation: Dict[str, Any],
                                 legal_actions: List[int],
                                 target_agent,
                                 info_set_manager: Optional[InfoSetManager],
                                 use_external_regrets: bool) -> int:
        """
        Get best response action (simplified implementation).
        
        For a full implementation, this would solve for the optimal counter-strategy.
        Here we use a simplified heuristic approach.
        """
        # Simplified best response: analyze target strategy and choose counter
        try:
            # Get target agent's strategy for this state
            target_strategy = self._get_target_strategy(
                target_agent,
                observation,
                legal_actions,
                info_set_manager,
                use_external_regrets
            )
            
            if target_strategy is not None:
                # Simple counter-strategy heuristics
                legal_strategy = target_strategy[legal_actions]
                
                # If target plays very passively (high fold/check probability), be aggressive
                passive_actions = [0, 1]  # fold, check/call
                passive_prob = sum(legal_strategy[i] for i in range(len(legal_actions)) 
                                 if legal_actions[i] in passive_actions)
                
                if passive_prob > 0.7:
                    # Target is passive, choose aggressive action
                    aggressive_actions = [2, 3, 4]  # raises and all-in
                    for action in legal_actions:
                        if action in aggressive_actions:
                            return action
                
                # If target plays very aggressively, be more conservative
                aggressive_actions = [2, 3, 4]
                aggressive_prob = sum(legal_strategy[i] for i in range(len(legal_actions))
                                    if legal_actions[i] in aggressive_actions)
                
                if aggressive_prob > 0.6:
                    # Target is aggressive, choose conservative action
                    if 1 in legal_actions:  # check/call
                        return 1
                    elif 0 in legal_actions:  # fold
                        return 0
            
            # Default: random legal action
            return random.choice(legal_actions)
            
        except Exception as e:
            self.logger.debug(f"Error in best response action selection: {e}")
            return random.choice(legal_actions) if legal_actions else 0
    
    def _get_target_strategy_action(self,
                                   target_agent,
                                   observation: Dict[str, Any],
                                   legal_actions: List[int],
                                   info_set_manager: Optional[InfoSetManager],
                                   use_external_regrets: bool) -> int:
        """
        Get action from target agent's strategy.
        """
        try:
            strategy = self._get_target_strategy(
                target_agent,
                observation,
                legal_actions,
                info_set_manager,
                use_external_regrets
            )
            
            if strategy is not None:
                # Sample action according to strategy
                legal_strategy = strategy[legal_actions]
                if np.sum(legal_strategy) > 0:
                    legal_strategy = legal_strategy / np.sum(legal_strategy)
                    action_idx = np.random.choice(len(legal_actions), p=legal_strategy)
                    return legal_actions[action_idx]
            
            # Fallback: uniform random
            return random.choice(legal_actions)
            
        except Exception as e:
            self.logger.debug(f"Error getting target strategy action: {e}")
            return random.choice(legal_actions) if legal_actions else 0
    
    def _get_target_strategy(self,
                            target_agent,
                            observation: Dict[str, Any],
                            legal_actions: List[int],
                            info_set_manager: Optional[InfoSetManager],
                            use_external_regrets: bool) -> Optional[np.ndarray]:
        """
        Get strategy from target agent.
        """
        try:
            if use_external_regrets and info_set_manager:
                # Use external regret strategy
                info_set_key = self._generate_info_set_key(observation)
                return info_set_manager.get_strategy(info_set_key, legal_actions)
            else:
                # Use agent's network strategy
                if hasattr(target_agent, 'get_strategy'):
                    hole_cards = observation.get('hole_cards', [])
                    community_cards = observation.get('community_cards', [])
                    action_history = observation.get('action_history', [])
                    
                    return target_agent.get_strategy(
                        hole_cards=hole_cards,
                        community_cards=community_cards,
                        action_history=action_history,
                        legal_actions=legal_actions
                    )
                elif hasattr(target_agent, 'network'):
                    # Direct network access
                    hole_cards = observation.get('hole_cards', [])
                    community_cards = observation.get('community_cards', [])
                    action_history = observation.get('action_history', [])
                    
                    return target_agent.network.get_strategy(
                        hole_cards=hole_cards,
                        community_cards=community_cards,
                        action_history=action_history,
                        legal_actions=legal_actions
                    )
            
            return None
            
        except Exception as e:
            self.logger.debug(f"Error getting target strategy: {e}")
            return None
    
    def _generate_info_set_key(self, observation: Dict[str, Any]) -> str:
        """
        Generate information set key for exploitability calculation.
        
        Simplified version for best response calculation.
        """
        hole_cards = observation.get('hole_cards', [])
        community_cards = observation.get('community_cards', [])
        action_history = observation.get('action_history', [])
        
        # Simple key generation
        cards_str = ','.join(hole_cards + community_cards)
        actions_str = ','.join(map(str, action_history[-5:]))  # Last 5 actions
        
        return f"{cards_str}|{actions_str}"
    
    def get_exploitability_statistics(self, 
                                     recent_exploitabilities: List[float]) -> Dict[str, float]:
        """
        Get statistics about exploitability over time.
        
        Args:
            recent_exploitabilities: List of recent exploitability measurements
            
        Returns:
            Dictionary with exploitability statistics
        """
        if not recent_exploitabilities:
            return {
                'current': float('inf'),
                'average': float('inf'),
                'min': float('inf'),
                'max': float('inf'),
                'trend': 0.0,
                'improvement': 0.0
            }
        
        current = recent_exploitabilities[-1]
        average = np.mean(recent_exploitabilities)
        minimum = np.min(recent_exploitabilities)
        maximum = np.max(recent_exploitabilities)
        
        # Calculate trend (negative = improving)
        if len(recent_exploitabilities) >= 2:
            trend = recent_exploitabilities[-1] - recent_exploitabilities[-2]
        else:
            trend = 0.0
        
        # Calculate improvement from start
        if len(recent_exploitabilities) >= 2:
            improvement = recent_exploitabilities[0] - recent_exploitabilities[-1]
        else:
            improvement = 0.0
        
        return {
            'current': float(current),
            'average': float(average),
            'min': float(minimum),
            'max': float(maximum),
            'trend': float(trend),
            'improvement': float(improvement)
        }